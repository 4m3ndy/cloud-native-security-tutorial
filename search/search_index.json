{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"This KubeCon EU 2020 tutorial will get you off the ground with Kubernetes security basics, using live demos and examples to work through yourself. We\u2019ll start with possible attack vectors, to help you map out the threat model that applies to your cluster, so you can figure out where you need to focus your efforts for security. We\u2019ll show you how to compromise a deployment with a pod running with a known vulnerability. Once you\u2019ve had the attacker\u2019s eye-view, we\u2019ll walk you through the most important techniques and open source tools to prevent compromise. Sign up now!","title":"Overview"},{"location":"compromise/","text":"Run a compromised pod \u00b6 In this section we deliberately introduce a container image with a known vulnerability so that you can enjoy the experience of exploiting it! Run an Apache server vulnerable to Shellshock \u00b6 We're using a container with Shellshock , a vulnerability in bash that allows an attacker to remotely execute commands. To make it really easy, we're providing a Helm chart that installs the vulnerable deployment. Danger DO NOT RUN THIS IN A REAL CLUSTER! helm install shellshockable https://lizrice.github.io/shellshockable/shellshockable-0.1.0.tgz This will run a deployment with a single pod. It might take a few seconds to pull the image so check that it's up and running (your pod name will be different): kubectl get pods You'll see something like this: NAME READY STATUS RESTARTS AGE shellshockable-d5b7d44b4-9rpzd 1/1 Running 0 70s In a separate terminal window, run port forwarding so we can make curl requests to this pod. This maps localhost port 8081 to the pod's port 80. kubectl port-forward $(kubectl get pods -l app.kubernetes.io/name=shellshockable -o name) 8081:80 You can now use curl (or a browser) to view the web service running on this pod. For example, if you open localhost:8081 in your web browser, you'll see the Apache2 default welcome page. There is also a script that will return the words \"Regular, expected output\" at the address localhost:8081/cgi-bin/shockme.cgi . If you look at the source for the script you'll see that it's executed using bash . This container is using a compromised version of bash with the Shellshock vulnerability, that an attacker can exploit to execute commands. First let's see the regular text is returned if we use curl to make the HTTPS request: curl localhost:8081/cgi-bin/shockme.cgi This should show the response Regular, expected output You've seen the web server return content as expected. Now it's time to act like an attacker and exploit the Shellshock vulnerability. Exploit the vulnerability \u00b6 You can exploit Shellshock by passing a User-Agent header on the curl request with the -A parameter. This gets expanded as an environment variable by bash , and because of the Shellshock vulnerability, it can be used to execute arbitrary commands. For example curl -A \"() { :; }; echo \\\"Content-type: text/plain\\\"; echo; /bin/cat /etc/passwd\" localhost:8081/cgi-bin/shockme.cgi Instead of running the CGI script as normal, this reponds to the HTTP request with the contents of /etc/passwd ! Preventing this attack \u00b6 The safest way to prevent this attack is to ensure the vulnerable version of bash isn't included in the container image before it's deployed. This is done with container image scanning . Image scanning can only detect known, published vulnerabilities. You can also limit the likely damage of as-yet-unknown compromises by configuring containers to run more securely and using policies to enforce safer configuration. Optional questions and exercises \u00b6 If you have the time and interest, here are some additional things you might like to think about. Try running some other commands as if you were an attacker! For example, you could find out what environment variables are set see what user ID you're running as explore the contents of the filesystem to see what is available Where does this /etc/passwd file come from - the host or the container? What would happen if you mounted files from the host into this container? Take a look at the Dockerfile that builds the container image used in this example. It deliberately installed an old, vulnerable version of bash . Try running a deployment with an up-to-date version of apache2 without the vulnerability, and check that you can't exploit it in the same way.","title":"Compromise a pod!"},{"location":"compromise/#run-a-compromised-pod","text":"In this section we deliberately introduce a container image with a known vulnerability so that you can enjoy the experience of exploiting it!","title":"Run a compromised pod"},{"location":"compromise/#run-an-apache-server-vulnerable-to-shellshock","text":"We're using a container with Shellshock , a vulnerability in bash that allows an attacker to remotely execute commands. To make it really easy, we're providing a Helm chart that installs the vulnerable deployment. Danger DO NOT RUN THIS IN A REAL CLUSTER! helm install shellshockable https://lizrice.github.io/shellshockable/shellshockable-0.1.0.tgz This will run a deployment with a single pod. It might take a few seconds to pull the image so check that it's up and running (your pod name will be different): kubectl get pods You'll see something like this: NAME READY STATUS RESTARTS AGE shellshockable-d5b7d44b4-9rpzd 1/1 Running 0 70s In a separate terminal window, run port forwarding so we can make curl requests to this pod. This maps localhost port 8081 to the pod's port 80. kubectl port-forward $(kubectl get pods -l app.kubernetes.io/name=shellshockable -o name) 8081:80 You can now use curl (or a browser) to view the web service running on this pod. For example, if you open localhost:8081 in your web browser, you'll see the Apache2 default welcome page. There is also a script that will return the words \"Regular, expected output\" at the address localhost:8081/cgi-bin/shockme.cgi . If you look at the source for the script you'll see that it's executed using bash . This container is using a compromised version of bash with the Shellshock vulnerability, that an attacker can exploit to execute commands. First let's see the regular text is returned if we use curl to make the HTTPS request: curl localhost:8081/cgi-bin/shockme.cgi This should show the response Regular, expected output You've seen the web server return content as expected. Now it's time to act like an attacker and exploit the Shellshock vulnerability.","title":"Run an Apache server vulnerable to Shellshock"},{"location":"compromise/#exploit-the-vulnerability","text":"You can exploit Shellshock by passing a User-Agent header on the curl request with the -A parameter. This gets expanded as an environment variable by bash , and because of the Shellshock vulnerability, it can be used to execute arbitrary commands. For example curl -A \"() { :; }; echo \\\"Content-type: text/plain\\\"; echo; /bin/cat /etc/passwd\" localhost:8081/cgi-bin/shockme.cgi Instead of running the CGI script as normal, this reponds to the HTTP request with the contents of /etc/passwd !","title":"Exploit the vulnerability"},{"location":"compromise/#preventing-this-attack","text":"The safest way to prevent this attack is to ensure the vulnerable version of bash isn't included in the container image before it's deployed. This is done with container image scanning . Image scanning can only detect known, published vulnerabilities. You can also limit the likely damage of as-yet-unknown compromises by configuring containers to run more securely and using policies to enforce safer configuration.","title":"Preventing this attack"},{"location":"compromise/#optional-questions-and-exercises","text":"If you have the time and interest, here are some additional things you might like to think about. Try running some other commands as if you were an attacker! For example, you could find out what environment variables are set see what user ID you're running as explore the contents of the filesystem to see what is available Where does this /etc/passwd file come from - the host or the container? What would happen if you mounted files from the host into this container? Take a look at the Dockerfile that builds the container image used in this example. It deliberately installed an old, vulnerable version of bash . Try running a deployment with an up-to-date version of apache2 without the vulnerability, and check that you can't exploit it in the same way.","title":"Optional questions and exercises"},{"location":"conclusions/","text":"Conclusions \u00b6 We hope this tutorial has introduced you to some practical steps you can take to make your Kubernetes deployments more secure. Thank you for your time! Further reading \u00b6 You'll find more details and further resources about Kubernetes Security here including a link to download an electronic copy of our book here . If you'd like to dive into more technical details, you might also like to check out Liz's book on Container Security .","title":"Conclusion"},{"location":"conclusions/#conclusions","text":"We hope this tutorial has introduced you to some practical steps you can take to make your Kubernetes deployments more secure. Thank you for your time!","title":"Conclusions"},{"location":"conclusions/#further-reading","text":"You'll find more details and further resources about Kubernetes Security here including a link to download an electronic copy of our book here . If you'd like to dive into more technical details, you might also like to check out Liz's book on Container Security .","title":"Further reading"},{"location":"gitops/","text":"GitOps \u00b6 Concept \u00b6 Using Flux \u00b6 Using ArgoCD \u00b6","title":"GitOps"},{"location":"gitops/#gitops","text":"","title":"GitOps"},{"location":"gitops/#concept","text":"","title":"Concept"},{"location":"gitops/#using-flux","text":"","title":"Using Flux"},{"location":"gitops/#using-argocd","text":"","title":"Using ArgoCD"},{"location":"introduction/","text":"Introduction \u00b6 If you want to follow along with the hands-on examples in this tutorial, you'll need a Kubernetes cluster and Helm to get started. You'll find instructions in the Preparation page. Before we get to the examples, let's review some of the different ways that an attacker might compromise a Kubernetes cluster. Kubernetes attack vectors \u00b6 We won't have time to explore all of these in detail but we will be covering some of the most important topics. An attacker might take advantage of vulnerabilities in your application code. You can go a long way to address this by scanning container images for vulnerabilities If an attacker gets a foothold within a container, you want to prevent them from escaping the container to access the host or other components, by configuring container images with security in mind, and checking them with policies The Kubernetes components and APIs offer more potential surfaces for attack, so you want to checking your Kubernetes configuration for settings that might leave your deployment vulnerable. At the root of many security issues lies human error, as well as bad actors. You can limit human access to the cluster, and thus enhancing security using GitOps","title":"Introduction"},{"location":"introduction/#introduction","text":"If you want to follow along with the hands-on examples in this tutorial, you'll need a Kubernetes cluster and Helm to get started. You'll find instructions in the Preparation page. Before we get to the examples, let's review some of the different ways that an attacker might compromise a Kubernetes cluster.","title":"Introduction"},{"location":"introduction/#kubernetes-attack-vectors","text":"We won't have time to explore all of these in detail but we will be covering some of the most important topics. An attacker might take advantage of vulnerabilities in your application code. You can go a long way to address this by scanning container images for vulnerabilities If an attacker gets a foothold within a container, you want to prevent them from escaping the container to access the host or other components, by configuring container images with security in mind, and checking them with policies The Kubernetes components and APIs offer more potential surfaces for attack, so you want to checking your Kubernetes configuration for settings that might leave your deployment vulnerable. At the root of many security issues lies human error, as well as bad actors. You can limit human access to the cluster, and thus enhancing security using GitOps","title":"Kubernetes attack vectors"},{"location":"policies/","text":"Policies \u00b6 One important tool in the defense in depth strategy are policies. These define what is or is not allowed and part of it is usually an enforcement component. In this section we will have a look at a number of different policies and how you can apply them. In the context of policies, the concept of least privileges is an important one, so let's have a look at this for starters. Least privileges \u00b6 With least privileges we mean to equip someone or something with exactly the rights to carry out their or its task but not more. For example, if you consider a program that needs to read from a specific location in the filesystem then the operation ( read ) and the location (say, /data ) would determine what permissions are necessary. Conversely, said program would not need write access in addition and hence this would violate the least privileges principle. First off we start with the simple case of a Kubernetes security context, allowing you to specify runtime policies around privileges and access control. Preparation \u00b6 Let's create the cluster for it: kind create cluster --name cnsectut \\ --config res/security-context-cluster-config.yaml Using a security context \u00b6 We will be using an example from the Kubernetes docs so that you can read up on the details later on. First, launch the pod with a security context defined like so: kubectl apply -f https://raw.githubusercontent.com/k8s-sec/cloud-native-security-tutorial/master/res/pod-security-context.yaml Next, enter the pod: kubectl exec -it security-context -- sh And, in the pod, create a file as shown: echo something > /data/content ; cd /data && id What you see here is how the security context defined in the pod enforces file and group ownership. But who enforces the definition of security contexts? That is, how can you make sure that a developer creating a pod spec in fact thinks of this? Enter Pod Security Policies or PSP for short. Learn more about least privileges practices in the container runtime context via: rootlesscontaine.rs canihaznonprivilegedcontainers.info Clean up with kind delete cluster --name cnsectut when you're done exploring this topic. Network policies \u00b6 So runtime policies are fun, but not the only thing that matters: next up, we have a look at network policies. These kind of policies allow you to control the communication patterns in-cluster (between pods) and concerning the outside world (ingress and egress traffic): You might be surprised to learn that in Kubernetes by default all traffic (in-cluster and to/from the outside world) is allowed. That is, any pod can see and talk to any other pod by default as well as any connection to a pod running in your Kubernetes cluster. Preparation \u00b6 Let's create the cluster for the network policies walkthrough. The following can take a minute or two, depending on if you've pulled the container images before or doing it the first time (then it can take 10min or more): kind create cluster --name cnnp --config res/network-policy-cluster-config.yaml Next, install the Calico controller and custom resources (this is the CNI plugin that allows us to enforce the network policies): kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml Then, we need to patch the setup due to the fact we're using kind here (kudos to Alex for the patch instructions): kubectl -n kube-system set env daemonset/calico-node FELIX_IGNORELOOSERPF=true Now you can verify the setup, and note that it can take some 5 min until you see all pods in the Running state: kubectl -n kube-system get pods | grep calico-node Last but not least we install Ambassador as an ingress controller so that we can access to workloads from outside of the cluster: kubectl apply -f https://github.com/datawire/ambassador-operator/releases/latest/download/ambassador-operator-crds.yaml && \\ kubectl apply -n ambassador -f https://github.com/datawire/ambassador-operator/releases/latest/download/ambassador-operator-kind.yaml && \\ kubectl wait --timeout=180s -n ambassador --for=condition=deployed ambassadorinstallations/ambassador And with that we're ready to apply some network policies. Limit ingress traffic \u00b6 Now let's see network policies in action by creating a public-facing workload and define the communication paths. First off, we want to do all of the following in a dedicated namespace called npdemo : kubectl create ns npdemo Now, create the workload (deployment, service, ingress): kubectl -n npdemo apply -f res/np-workload.yaml When you now query the endpoint defined by the ingress resource you deployed in the previous step you will find that it works as expected (remember: by default, all is allowed/open): curl localhost/api Now we shut down all traffic with: kubectl -n npdemo apply -f res/deny-all.yaml And try again: curl localhost/api As we'd have hoped and expected the access is now denied (might need to give it a second or so until the change is picked up). But now, how do we allow traffic to the frontend (represented by the NGINX web server)? Well, we define another network policy that allows ingress to stuff labelled with role=frontend : kubectl -n npdemo apply -f res/allow-frontend.yaml && \\ kubectl -n npdemo label pods --selector=app=nginx role=frontend And now it should work again: curl localhost/api Learn more about network policies via: Exploring Network Policies in Kubernetes Best Practices for Kubernetes Network Policies Securing Kubernetes Cluster Networking Clean up with kind delete cluster --name cnnp when you're done exploring this topic. General purpose policies \u00b6 We have now seen container runtime policies as well as network policies in action. You should by now have an idea what policies are and how to go about defining and enforcing them. But did you notice one thing: for every type of policy, we had a different mechanism (and mind you, we only had a look at two types). Also, when you want to introduce further policies, for example, company ones or maybe stuff you need to do to be compliant with some regulatory framework such as PCI DSS. How do you deal with this in the context of containers? Meet the CNCF Open Policy Agent (OPA) project. OPA (pronounced \"oh-pa\") is a general-purpose policy engine that comes with a powerful rule-based policy language called Rego (pronounced \"ray-go\"). Rego takes any kind of JSON data as input and matches against a set of rules. It also comes with a long list of built-in functions, with from simple string manipulation stuff like strings.replace_n(patterns, string) to fancy things such as crypto.x509.parse_certificates(string) . Enough theory, let's jump into the deep end using the OPA Rego playground . OPA in action \u00b6 Let's say you have the following input data, which is an array of timestamped entries: [ { \"msg\": \"within a week\", \"timestamp\": \"2020-03-27T12:00:00Z\" }, { \"msg\": \"same year\", \"timestamp\": \"2020-10-09T12:00:00Z\" }, { \"msg\": \"last year\", \"timestamp\": \"2019-12-24T12:00:00Z\" } ] So how can we check if a given entry is within a certain time window? For example, you might require that a certain commit is not older than a week. The following Rego file defines the policy we want to enforce (with a fixed point in time 2020-04-01T12:00:00Z as a reference): package play ts_reference := time.parse_rfc3339_ns(\"2020-04-01T12:00:00Z\") time_window_check[results] { some i msg := input[i].msg ts := time.parse_rfc3339_ns(input[i].timestamp) results := { \"same_year\" : same_year(ts), \"within_a_week\" : within_a_week(ts), \"message\": msg, } } same_year(ts) { [c_y, c_m, c_d] := time.date(ts_reference) [ts_y, ts_m, ts_d] := time.date(ts) ts_y == c_y } within_a_week(ts) { a_week := 60 * 60 * 24 * 7 diff := ts_reference/1000/1000/1000 - ts/1000/1000/1000 diff < a_week ts_reference > ts } Applying above Rego rule set to the input data, that is, querying for time_window_check[results] yields: Found 1 result in 872.098 \u00b5s. { \"time_window_check\": [ { \"message\": \"within a week\", \"same_year\": true, \"within_a_week\": true } ], \"ts_reference\": 1585742400000000000 } You can try this online yourself via the prepared playground example . OPA Gatekeeper \u00b6 Now that you have an idea what OPA and Rego is you might wonder how hard it is to use OPA/Rego in the context of Kubernetes. Turns out that writing, testing, and enforcing these Rego rules is relatively hard and something that you don't want to push onto individual folks. Good news is that the community came together to tackle this problem in the form of the Gatekeeper project. The Gatekeeper project solves the challenge of having to write and enforce Rego rules by using the Kubernetes-native extension points of custom resources and dynamic admission control . As an end-user it's as simple as follows to use OPA with Gatekeeper. After installing Gatekeeper, define and apply a custom resource like the following: apiVersion: constraints.gatekeeper.sh/v1beta1 kind: K8sRequiredLabels metadata: name: test-ns-label spec: match: kinds: - apiGroups: [\"\"] kinds: [\"Namespace\"] parameters: labels: [\"test\"] This constraint above requires that all namespaces MUST have a label test . But where is the Rego rule set I hear you ask? Gatekeeper employs a separation of duties approach where (someone other than the end-user) defines a template like so: apiVersion: templates.gatekeeper.sh/v1beta1 kind: ConstraintTemplate metadata: name: k8srequiredlabels spec: crd: spec: names: kind: K8sRequiredLabels listKind: K8sRequiredLabelsList plural: k8srequiredlabels singular: k8srequiredlabels validation: # Schema for the `parameters` field openAPIV3Schema: properties: labels: type: array items: string targets: - target: admission.k8s.gatekeeper.sh rego: | package k8srequiredlabels violation[{\"msg\": msg, \"details\": {\"missing_labels\": missing}}] { provided := {label | input.review.object.metadata.labels[label]} required := {label | label := input.parameters.labels[_]} missing := required - provided count(missing) > 0 msg := sprintf(\"you must provide labels: %v\", [missing]) } Above template effectively represents a custom resource definition that Gatekeeper understands and can enforce via an Webhook registered in the Kubernetes API server. Learn more about OPA and Gatekeeper via: Introducing Policy As Code: The Open Policy Agent (OPA) OPA blog Styra Academy OPA Gatekeeper: Policy and Governance for Kubernetes CNCF webinar: Kubernetes with OPA Gatekeeper","title":"Policies"},{"location":"policies/#policies","text":"One important tool in the defense in depth strategy are policies. These define what is or is not allowed and part of it is usually an enforcement component. In this section we will have a look at a number of different policies and how you can apply them. In the context of policies, the concept of least privileges is an important one, so let's have a look at this for starters.","title":"Policies"},{"location":"policies/#least-privileges","text":"With least privileges we mean to equip someone or something with exactly the rights to carry out their or its task but not more. For example, if you consider a program that needs to read from a specific location in the filesystem then the operation ( read ) and the location (say, /data ) would determine what permissions are necessary. Conversely, said program would not need write access in addition and hence this would violate the least privileges principle. First off we start with the simple case of a Kubernetes security context, allowing you to specify runtime policies around privileges and access control.","title":"Least privileges"},{"location":"policies/#preparation","text":"Let's create the cluster for it: kind create cluster --name cnsectut \\ --config res/security-context-cluster-config.yaml","title":"Preparation"},{"location":"policies/#using-a-security-context","text":"We will be using an example from the Kubernetes docs so that you can read up on the details later on. First, launch the pod with a security context defined like so: kubectl apply -f https://raw.githubusercontent.com/k8s-sec/cloud-native-security-tutorial/master/res/pod-security-context.yaml Next, enter the pod: kubectl exec -it security-context -- sh And, in the pod, create a file as shown: echo something > /data/content ; cd /data && id What you see here is how the security context defined in the pod enforces file and group ownership. But who enforces the definition of security contexts? That is, how can you make sure that a developer creating a pod spec in fact thinks of this? Enter Pod Security Policies or PSP for short. Learn more about least privileges practices in the container runtime context via: rootlesscontaine.rs canihaznonprivilegedcontainers.info Clean up with kind delete cluster --name cnsectut when you're done exploring this topic.","title":"Using a security context"},{"location":"policies/#network-policies","text":"So runtime policies are fun, but not the only thing that matters: next up, we have a look at network policies. These kind of policies allow you to control the communication patterns in-cluster (between pods) and concerning the outside world (ingress and egress traffic): You might be surprised to learn that in Kubernetes by default all traffic (in-cluster and to/from the outside world) is allowed. That is, any pod can see and talk to any other pod by default as well as any connection to a pod running in your Kubernetes cluster.","title":"Network policies"},{"location":"policies/#preparation_1","text":"Let's create the cluster for the network policies walkthrough. The following can take a minute or two, depending on if you've pulled the container images before or doing it the first time (then it can take 10min or more): kind create cluster --name cnnp --config res/network-policy-cluster-config.yaml Next, install the Calico controller and custom resources (this is the CNI plugin that allows us to enforce the network policies): kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml Then, we need to patch the setup due to the fact we're using kind here (kudos to Alex for the patch instructions): kubectl -n kube-system set env daemonset/calico-node FELIX_IGNORELOOSERPF=true Now you can verify the setup, and note that it can take some 5 min until you see all pods in the Running state: kubectl -n kube-system get pods | grep calico-node Last but not least we install Ambassador as an ingress controller so that we can access to workloads from outside of the cluster: kubectl apply -f https://github.com/datawire/ambassador-operator/releases/latest/download/ambassador-operator-crds.yaml && \\ kubectl apply -n ambassador -f https://github.com/datawire/ambassador-operator/releases/latest/download/ambassador-operator-kind.yaml && \\ kubectl wait --timeout=180s -n ambassador --for=condition=deployed ambassadorinstallations/ambassador And with that we're ready to apply some network policies.","title":"Preparation"},{"location":"policies/#limit-ingress-traffic","text":"Now let's see network policies in action by creating a public-facing workload and define the communication paths. First off, we want to do all of the following in a dedicated namespace called npdemo : kubectl create ns npdemo Now, create the workload (deployment, service, ingress): kubectl -n npdemo apply -f res/np-workload.yaml When you now query the endpoint defined by the ingress resource you deployed in the previous step you will find that it works as expected (remember: by default, all is allowed/open): curl localhost/api Now we shut down all traffic with: kubectl -n npdemo apply -f res/deny-all.yaml And try again: curl localhost/api As we'd have hoped and expected the access is now denied (might need to give it a second or so until the change is picked up). But now, how do we allow traffic to the frontend (represented by the NGINX web server)? Well, we define another network policy that allows ingress to stuff labelled with role=frontend : kubectl -n npdemo apply -f res/allow-frontend.yaml && \\ kubectl -n npdemo label pods --selector=app=nginx role=frontend And now it should work again: curl localhost/api Learn more about network policies via: Exploring Network Policies in Kubernetes Best Practices for Kubernetes Network Policies Securing Kubernetes Cluster Networking Clean up with kind delete cluster --name cnnp when you're done exploring this topic.","title":"Limit ingress traffic"},{"location":"policies/#general-purpose-policies","text":"We have now seen container runtime policies as well as network policies in action. You should by now have an idea what policies are and how to go about defining and enforcing them. But did you notice one thing: for every type of policy, we had a different mechanism (and mind you, we only had a look at two types). Also, when you want to introduce further policies, for example, company ones or maybe stuff you need to do to be compliant with some regulatory framework such as PCI DSS. How do you deal with this in the context of containers? Meet the CNCF Open Policy Agent (OPA) project. OPA (pronounced \"oh-pa\") is a general-purpose policy engine that comes with a powerful rule-based policy language called Rego (pronounced \"ray-go\"). Rego takes any kind of JSON data as input and matches against a set of rules. It also comes with a long list of built-in functions, with from simple string manipulation stuff like strings.replace_n(patterns, string) to fancy things such as crypto.x509.parse_certificates(string) . Enough theory, let's jump into the deep end using the OPA Rego playground .","title":"General purpose policies"},{"location":"policies/#opa-in-action","text":"Let's say you have the following input data, which is an array of timestamped entries: [ { \"msg\": \"within a week\", \"timestamp\": \"2020-03-27T12:00:00Z\" }, { \"msg\": \"same year\", \"timestamp\": \"2020-10-09T12:00:00Z\" }, { \"msg\": \"last year\", \"timestamp\": \"2019-12-24T12:00:00Z\" } ] So how can we check if a given entry is within a certain time window? For example, you might require that a certain commit is not older than a week. The following Rego file defines the policy we want to enforce (with a fixed point in time 2020-04-01T12:00:00Z as a reference): package play ts_reference := time.parse_rfc3339_ns(\"2020-04-01T12:00:00Z\") time_window_check[results] { some i msg := input[i].msg ts := time.parse_rfc3339_ns(input[i].timestamp) results := { \"same_year\" : same_year(ts), \"within_a_week\" : within_a_week(ts), \"message\": msg, } } same_year(ts) { [c_y, c_m, c_d] := time.date(ts_reference) [ts_y, ts_m, ts_d] := time.date(ts) ts_y == c_y } within_a_week(ts) { a_week := 60 * 60 * 24 * 7 diff := ts_reference/1000/1000/1000 - ts/1000/1000/1000 diff < a_week ts_reference > ts } Applying above Rego rule set to the input data, that is, querying for time_window_check[results] yields: Found 1 result in 872.098 \u00b5s. { \"time_window_check\": [ { \"message\": \"within a week\", \"same_year\": true, \"within_a_week\": true } ], \"ts_reference\": 1585742400000000000 } You can try this online yourself via the prepared playground example .","title":"OPA in action"},{"location":"policies/#opa-gatekeeper","text":"Now that you have an idea what OPA and Rego is you might wonder how hard it is to use OPA/Rego in the context of Kubernetes. Turns out that writing, testing, and enforcing these Rego rules is relatively hard and something that you don't want to push onto individual folks. Good news is that the community came together to tackle this problem in the form of the Gatekeeper project. The Gatekeeper project solves the challenge of having to write and enforce Rego rules by using the Kubernetes-native extension points of custom resources and dynamic admission control . As an end-user it's as simple as follows to use OPA with Gatekeeper. After installing Gatekeeper, define and apply a custom resource like the following: apiVersion: constraints.gatekeeper.sh/v1beta1 kind: K8sRequiredLabels metadata: name: test-ns-label spec: match: kinds: - apiGroups: [\"\"] kinds: [\"Namespace\"] parameters: labels: [\"test\"] This constraint above requires that all namespaces MUST have a label test . But where is the Rego rule set I hear you ask? Gatekeeper employs a separation of duties approach where (someone other than the end-user) defines a template like so: apiVersion: templates.gatekeeper.sh/v1beta1 kind: ConstraintTemplate metadata: name: k8srequiredlabels spec: crd: spec: names: kind: K8sRequiredLabels listKind: K8sRequiredLabelsList plural: k8srequiredlabels singular: k8srequiredlabels validation: # Schema for the `parameters` field openAPIV3Schema: properties: labels: type: array items: string targets: - target: admission.k8s.gatekeeper.sh rego: | package k8srequiredlabels violation[{\"msg\": msg, \"details\": {\"missing_labels\": missing}}] { provided := {label | input.review.object.metadata.labels[label]} required := {label | label := input.parameters.labels[_]} missing := required - provided count(missing) > 0 msg := sprintf(\"you must provide labels: %v\", [missing]) } Above template effectively represents a custom resource definition that Gatekeeper understands and can enforce via an Webhook registered in the Kubernetes API server. Learn more about OPA and Gatekeeper via: Introducing Policy As Code: The Open Policy Agent (OPA) OPA blog Styra Academy OPA Gatekeeper: Policy and Governance for Kubernetes CNCF webinar: Kubernetes with OPA Gatekeeper","title":"OPA Gatekeeper"},{"location":"preparation/","text":"Preparation \u00b6 Create a Kubernetes cluster \u00b6 To follow along with the practical examples in this tutorial you'll need a Kubernetes cluster that you can experiment with. Since at times you will be deploying insecure code, please don't use your production cluster! You can run a cluster locally on your laptop, for example, we will be using Kubernetes IN Docker or kind for short along with Helm to install and run apps on the cluster. Install kind \u00b6 You can skip this step if you already have an up-to-date installation of kind . On MacOS, using Homebrew: brew install kind On MacOS/Linux: curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.8.1/kind-$(uname)-amd64 chmod +x ./kind mv ./kind /some-dir-in-your-PATH/kind On Windows, using Chocolatey: choco install kind For more details see the kind quickstart guide . Create cluster \u00b6 To create a kind cluster (effectively a bunch of Docker containers running locally): kind create cluster Once it's up and running, check that you can see the worker node is up and running: kubectl get nodes This should show something like this: NAME STATUS ROLES AGE VERSION kind-control-plane Ready master 78m v1.18.2 Great! You have a Kubernetes cluster running locally that you can experiment with. Where appropriate, we will create dedicated kind clusters with certain configurations. Install Helm \u00b6 If you don't already have Helm on your laptop, you'll want to install that too. Find full instructions in the Helm documentation or here is a quick guide: On MacOS, using Homebrew: brew install helm On MacOS/Linux: curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 chmod +x get_helm.sh ./get_helm.sh On Windows, using Chocolatey: choco install kubernetes-helm Note If you have a fresh Kind installation there won't be any Helm charts installed yet, so a helm ls will return an empty list: $ helm ls NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION Clone repo \u00b6 In certain sections, you'll be using certain configuration files and Kubernetes manifests as defined in the res/ directory of our GitHub repository. So, either download the content of said repository or simply clone the repo with: git clone https://github.com/k8s-sec/cloud-native-security-tutorial.git Unless we say otherwise, we assume cloud-native-security-tutorial/ to be the base directory, going forward. OK and with this you're all set and ready to go!","title":"Preparation"},{"location":"preparation/#preparation","text":"","title":"Preparation"},{"location":"preparation/#create-a-kubernetes-cluster","text":"To follow along with the practical examples in this tutorial you'll need a Kubernetes cluster that you can experiment with. Since at times you will be deploying insecure code, please don't use your production cluster! You can run a cluster locally on your laptop, for example, we will be using Kubernetes IN Docker or kind for short along with Helm to install and run apps on the cluster.","title":"Create a Kubernetes cluster"},{"location":"preparation/#install-kind","text":"You can skip this step if you already have an up-to-date installation of kind . On MacOS, using Homebrew: brew install kind On MacOS/Linux: curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.8.1/kind-$(uname)-amd64 chmod +x ./kind mv ./kind /some-dir-in-your-PATH/kind On Windows, using Chocolatey: choco install kind For more details see the kind quickstart guide .","title":"Install kind"},{"location":"preparation/#create-cluster","text":"To create a kind cluster (effectively a bunch of Docker containers running locally): kind create cluster Once it's up and running, check that you can see the worker node is up and running: kubectl get nodes This should show something like this: NAME STATUS ROLES AGE VERSION kind-control-plane Ready master 78m v1.18.2 Great! You have a Kubernetes cluster running locally that you can experiment with. Where appropriate, we will create dedicated kind clusters with certain configurations.","title":"Create cluster"},{"location":"preparation/#install-helm","text":"If you don't already have Helm on your laptop, you'll want to install that too. Find full instructions in the Helm documentation or here is a quick guide: On MacOS, using Homebrew: brew install helm On MacOS/Linux: curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 chmod +x get_helm.sh ./get_helm.sh On Windows, using Chocolatey: choco install kubernetes-helm Note If you have a fresh Kind installation there won't be any Helm charts installed yet, so a helm ls will return an empty list: $ helm ls NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION","title":"Install Helm"},{"location":"preparation/#clone-repo","text":"In certain sections, you'll be using certain configuration files and Kubernetes manifests as defined in the res/ directory of our GitHub repository. So, either download the content of said repository or simply clone the repo with: git clone https://github.com/k8s-sec/cloud-native-security-tutorial.git Unless we say otherwise, we assume cloud-native-security-tutorial/ to be the base directory, going forward. OK and with this you're all set and ready to go!","title":"Clone repo"},{"location":"scanning/","text":"Vulnerability scanning \u00b6 The Shellshock vulnerability demonstrated is a serious vulnerability, but it's just one of thousands of known, exploitable vulnerabilities that range from negligible right up to critical in severity. Most organizations have a policy of not deploying software with known vulnerabilities over a certain severity threshold. A vulnerability scanner can show you which vulnerabilities are present in your container images. This data can inform policies to prevent deploying vulnerable containers that an attacker could exploit. There are several solutions for vulnerability scanning available, and in this workshop we will use the open source scanner Trivy . Running Trivy on the desktop \u00b6 Let's start by installing Trivy so we can run it locally. Install Trivy \u00b6 On MacOS using Homebrew: brew install aquasecurity/trivy/trivy On MacOS / Linux: curl -sfL https://raw.githubusercontent.com/aquasecurity/trivy/master/contrib/install.sh | sh -s -- -b /usr/local/bin Trivy does not have a native binary for Windows, but you can run Trivy as a container . For more details see the installation instructions . Run Trivy to scan images \u00b6 To scan an image simply run trivy image <image> . For example, to find the vulnerabilities in the image used in the Shellshock-compromised pod, run this: trivy image lizrice/shellshockable:0.1.0 The Shellshock vulnerability is classed as HIGH severity, and it has the identifier CVE-2014-6271 . If you are seeing a lot of results you might want to filter out the lower severity results. For example: trivy image --severity=CRITICAL,HIGH,MEDIUM lizrice/shellshockable:0.1.0 For comparison, try this version of the image that does not have the Shellshock vulnerability: trivy image --severity=CRITICAL,HIGH,MEDIUM lizrice/shellshockable:0.3.0 Now try Trivy on your favourite images! Including vulnerability scanning in CI/CD \u00b6 One way to \"shift left\" security is to include vulnerability scanning as an automatic step in CI/CD. Your build steps might be something like this: Compile code Run tests Build Dockerfile into container image Scan image for vulnerabilities Push image to container registry If any step fails, the pipeline is stopped - so if a vulnerability is found, the scanning stage fails, and the image doesn't get pushed to the registry. Trivy has some settings that make it a great fit in CI/CD systems: --light downloads a smaller vulnerability database that omits some details such as the text description of each vulnerability --severity=CRITICAL,HIGH,MEDIUM only checks for vulnerabilities of MEDIUM or higher severity --exit-code=1 returns 1 if any vulnerabilties are found (above the threshold defined by --severity ). This non-zero exit code is interpreted as a failure by the CI/CD system. Take a look at these examples for integrating vulnerability scanning into various CI/CD systems . Scanning as part of admission control \u00b6 To prevent deploying a container image with known vulnerabilities, you can use an admission controller that either scans the image, or retrieves the result from a previous scan and denies admission if the image is vulnerable. Teppei Fukuda has a session at KubeCon this week showing the use of OPA to make policy checks at the admission control stage. Scanning running Kubernetes workloads \u00b6 Scanning in the CI/CD pipeline can prevent you from storing a vulnerable image in the registry, and admission control can prevent you from deploying an image with known vulnerabilities. But you may also want the ability to check the container images used by your live workloads. New vulnerabilities are found all the time. An image that you scan today might not have any vulnerablities, but a researcher may find a new issue that means the same image will fail the scan tomorrow. Starboard is a tool for running security tools, including Trivy, within your Kubernetes cluster. This is an easy way to create and view scans of the container images used by your running workloads. Want to give it a try? Install Starboard \u00b6 If you don't already have the krew plugin manager installed, follow the instructions here Install Starboard as a kubectl plugin: kubectl krew install starboard Check that it's working kubectl starboard help On Mac you may get warnings about not being able to verify the developer - see this guide to resolve them . Initialize Starboard kubectl starboard init Check that this has created some CRDs kubectl get crds Find vulnerabilities \u00b6 Starboard uses Trivy to populate some vulnerabilities CRs that relate to running workloads. Let's find the vulnerabilities in the deployment we created for the Shellshock demo. kubectl starboard find vulns --delete-scan-job = false deployment/shellshockable This will take a few seconds. The --delete-scan-job=false parameter allows us to inspect the job that Starboard creates to run Trivy. But first let's look at the results: kubectl starboard get vulns deployment/shellshockable This outputs all the vulnerability information (including the details about Shellshock, with ID CVE-2014-6271). These details have been stored in a vulns resource that you can view with a regular kubectl get command: kubectl get vulns --show-labels The output should look something like this: NAME AGE LABELS c9b156db-ab92-4b32-a006-efc54312a8c1 4m14s starboard.container.name=shellshockable,starboard.resource.kind=Deployment,starboard.resource.name=shellshockable,starboard.resource.namespace=default The labels indicate which resource this vulnerability report applies to. TODO! On master, but not yet released as a binary, we have additional info with -o wide As mentioned above, Starboard creates a Kubernetes job that runs Trivy over the container images defined for the workload. This job lives in a namespace called starboard . In this example, the job finds that the shellshockable deployment uses the container image lizrice/shellshockable:0.1.0 , and runs Trivy over that image. If you specified --delete-scan-job=false you can inspect the completed job. kubectl describe $( kubectl get jobs -n starboard -o name ) -n starboard Within this output you can see that the job ran the trivy command, and the last argument to that command was the image name lizrice/shellshockable:0.1.0 . ... Containers: shellshockable: Image: docker.io/aquasec/trivy:0.9.1 Port: <none> Host Port: <none> Command: trivy Args: --skip-update --cache-dir /var/lib/trivy --no-progress --format json lizrice/shellshockable:0.1.0 ... View in Octant \u00b6 Starboard makes it easy to run Trivy over your running workloads, and coming soon, there will be an operator that will watch for new workloads and automatically run scans over them. Today, there is also an Octant plugin so you can view the vulnerability results through the Octant UI. You can check out the instructions for this here if you want to try it out.","title":"Scanning"},{"location":"scanning/#vulnerability-scanning","text":"The Shellshock vulnerability demonstrated is a serious vulnerability, but it's just one of thousands of known, exploitable vulnerabilities that range from negligible right up to critical in severity. Most organizations have a policy of not deploying software with known vulnerabilities over a certain severity threshold. A vulnerability scanner can show you which vulnerabilities are present in your container images. This data can inform policies to prevent deploying vulnerable containers that an attacker could exploit. There are several solutions for vulnerability scanning available, and in this workshop we will use the open source scanner Trivy .","title":"Vulnerability scanning"},{"location":"scanning/#running-trivy-on-the-desktop","text":"Let's start by installing Trivy so we can run it locally.","title":"Running Trivy on the desktop"},{"location":"scanning/#install-trivy","text":"On MacOS using Homebrew: brew install aquasecurity/trivy/trivy On MacOS / Linux: curl -sfL https://raw.githubusercontent.com/aquasecurity/trivy/master/contrib/install.sh | sh -s -- -b /usr/local/bin Trivy does not have a native binary for Windows, but you can run Trivy as a container . For more details see the installation instructions .","title":"Install Trivy"},{"location":"scanning/#run-trivy-to-scan-images","text":"To scan an image simply run trivy image <image> . For example, to find the vulnerabilities in the image used in the Shellshock-compromised pod, run this: trivy image lizrice/shellshockable:0.1.0 The Shellshock vulnerability is classed as HIGH severity, and it has the identifier CVE-2014-6271 . If you are seeing a lot of results you might want to filter out the lower severity results. For example: trivy image --severity=CRITICAL,HIGH,MEDIUM lizrice/shellshockable:0.1.0 For comparison, try this version of the image that does not have the Shellshock vulnerability: trivy image --severity=CRITICAL,HIGH,MEDIUM lizrice/shellshockable:0.3.0 Now try Trivy on your favourite images!","title":"Run Trivy to scan images"},{"location":"scanning/#including-vulnerability-scanning-in-cicd","text":"One way to \"shift left\" security is to include vulnerability scanning as an automatic step in CI/CD. Your build steps might be something like this: Compile code Run tests Build Dockerfile into container image Scan image for vulnerabilities Push image to container registry If any step fails, the pipeline is stopped - so if a vulnerability is found, the scanning stage fails, and the image doesn't get pushed to the registry. Trivy has some settings that make it a great fit in CI/CD systems: --light downloads a smaller vulnerability database that omits some details such as the text description of each vulnerability --severity=CRITICAL,HIGH,MEDIUM only checks for vulnerabilities of MEDIUM or higher severity --exit-code=1 returns 1 if any vulnerabilties are found (above the threshold defined by --severity ). This non-zero exit code is interpreted as a failure by the CI/CD system. Take a look at these examples for integrating vulnerability scanning into various CI/CD systems .","title":"Including vulnerability scanning in CI/CD"},{"location":"scanning/#scanning-as-part-of-admission-control","text":"To prevent deploying a container image with known vulnerabilities, you can use an admission controller that either scans the image, or retrieves the result from a previous scan and denies admission if the image is vulnerable. Teppei Fukuda has a session at KubeCon this week showing the use of OPA to make policy checks at the admission control stage.","title":"Scanning as part of admission control"},{"location":"scanning/#scanning-running-kubernetes-workloads","text":"Scanning in the CI/CD pipeline can prevent you from storing a vulnerable image in the registry, and admission control can prevent you from deploying an image with known vulnerabilities. But you may also want the ability to check the container images used by your live workloads. New vulnerabilities are found all the time. An image that you scan today might not have any vulnerablities, but a researcher may find a new issue that means the same image will fail the scan tomorrow. Starboard is a tool for running security tools, including Trivy, within your Kubernetes cluster. This is an easy way to create and view scans of the container images used by your running workloads. Want to give it a try?","title":"Scanning running Kubernetes workloads"},{"location":"scanning/#install-starboard","text":"If you don't already have the krew plugin manager installed, follow the instructions here Install Starboard as a kubectl plugin: kubectl krew install starboard Check that it's working kubectl starboard help On Mac you may get warnings about not being able to verify the developer - see this guide to resolve them . Initialize Starboard kubectl starboard init Check that this has created some CRDs kubectl get crds","title":"Install Starboard"},{"location":"scanning/#find-vulnerabilities","text":"Starboard uses Trivy to populate some vulnerabilities CRs that relate to running workloads. Let's find the vulnerabilities in the deployment we created for the Shellshock demo. kubectl starboard find vulns --delete-scan-job = false deployment/shellshockable This will take a few seconds. The --delete-scan-job=false parameter allows us to inspect the job that Starboard creates to run Trivy. But first let's look at the results: kubectl starboard get vulns deployment/shellshockable This outputs all the vulnerability information (including the details about Shellshock, with ID CVE-2014-6271). These details have been stored in a vulns resource that you can view with a regular kubectl get command: kubectl get vulns --show-labels The output should look something like this: NAME AGE LABELS c9b156db-ab92-4b32-a006-efc54312a8c1 4m14s starboard.container.name=shellshockable,starboard.resource.kind=Deployment,starboard.resource.name=shellshockable,starboard.resource.namespace=default The labels indicate which resource this vulnerability report applies to. TODO! On master, but not yet released as a binary, we have additional info with -o wide As mentioned above, Starboard creates a Kubernetes job that runs Trivy over the container images defined for the workload. This job lives in a namespace called starboard . In this example, the job finds that the shellshockable deployment uses the container image lizrice/shellshockable:0.1.0 , and runs Trivy over that image. If you specified --delete-scan-job=false you can inspect the completed job. kubectl describe $( kubectl get jobs -n starboard -o name ) -n starboard Within this output you can see that the job ran the trivy command, and the last argument to that command was the image name lizrice/shellshockable:0.1.0 . ... Containers: shellshockable: Image: docker.io/aquasec/trivy:0.9.1 Port: <none> Host Port: <none> Command: trivy Args: --skip-update --cache-dir /var/lib/trivy --no-progress --format json lizrice/shellshockable:0.1.0 ...","title":"Find vulnerabilities"},{"location":"scanning/#view-in-octant","text":"Starboard makes it easy to run Trivy over your running workloads, and coming soon, there will be an operator that will watch for new workloads and automatically run scans over them. Today, there is also an Octant plugin so you can view the vulnerability results through the Octant UI. You can check out the instructions for this here if you want to try it out.","title":"View in Octant"},{"location":"settings/","text":"Using secure Kubernetes settings \u00b6 When you install Kubernetes, there are numerous configuration settings that can affect security. In this section you'll learn about checking the settings in your cluster against the best practices advised by the Center for Internet Security. The CIS Kubernetes Benchmark \u00b6 The CIS have many different benchmarks recommending how to configure software components with security best practices in mind. One such benchmark is for Kubernetes - in fact there are several editions for managed version of Kubernetes, like EKS or GKE, as well as for upstream Kubernetes installations. There are hundreds of recommendations in these benchmarks, so running them manually on every node would be a time-consuming process. Running benchmark checks with kube-bench \u00b6 The open source tool kube-bench makes it easy to run the tests defined in the CIS Kubernetes benchmark. In this tutorial, you will use kube-bench to identify some insecure Kubernetes settings, and you'll remediate one of the settings to turn a failing test into a pass. You could run kube-bench in a cluster of your choice but for this tutorial we are showing it running in a kind (Kubernetes in Docker) single-node cluster that runs on your laptop as a Docker container. Run kube-bench on the kind cluster \u00b6 Danger What we are about to do is TERRIBLE practice but it makes it easier to write a platform-independent set of instructions for this tutorial. Never run YAML directly from the internet like this in your production cluster - check what's in it first! Create the kube-bench job \u00b6 kubectl apply -f https://raw.githubusercontent.com/aquasecurity/kube-bench/master/job.yaml You can watch the job until it has completed: kubectl get jobs --watch Hit Ctrl-C once the job has finished. Get the job output from the logs \u00b6 The job applies the label app: kube-bench to the pod, so you can easily retrieve the logs like this: kubectl logs $(kubectl get pods -l app=kube-bench -o name) Scroll back through the logs to see how it is divided into sections, each with its own set of results, remediation recommendations, and a summary. Most of the tests pass but there are a few results marked with [WARN] or [FAIL] [FAIL] means that the test failed [WARN] indicates that you need to do something manually to verify whether the test should pass or not. For more detail on the output check the kube-bench documentation . Remediate a test \u00b6 Note This tutorial was written using Kubernetes 1.18.2 and testing against the CIS Kubernetes Benchmark v1.5.1. If you are using a later version of Kubernetes, it's possible that the default configuration settings have changed and the results you get might not match what is described here. Scroll back through the results to find the result and (further down the results) the remediation information for the test 4.2.6. ... [FAIL] 4.2.6 Ensure that the --protect-kernel-defaults argument is set to true (Scored) ... 4.2.6 If using a Kubelet config file, edit the file to set protectKernelDefaults: true. If using command line arguments, edit the kubelet service file /etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and set the below parameter in KUBELET_SYSTEM_PODS_ARGS variable. --protect-kernel-defaults=true Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Kind uses a kubelet configuration file that lives at /var/lib/kubelet/config.yaml , so only the first line of the remediation text applies - you don't have to worry about editing the kubelet service file or restarting the service. Note When using kind, there is a Docker container running your control plane. This image for this container is based on Ubuntu so we can exec into the running container and then treat it much as if it were a virtual machine running a Kubernetes node. Edit the Kubelet configuration file \u00b6 First, open a shell into the kind container. docker exec -it kind-control-plane bash Assuming that it doesn't already have an editor installed, you can add one. apt-get update apt-get install vim Edit the Kubelet config file vi /var/lib/kubelet/config.yaml Add the line protectKernelDefaults: true so that the file looks something like this: apiVersion: kubelet.config.k8s.io/v1beta1 authentication: anonymous: enabled: false ... nodeStatusUpdateFrequency: 0s protectKernelDefaults: true rotateCertificates: true ... Save the file. The kubelet will spot that the configuration has changed and update itself, but meanwhile you can exit to leave the container so that you are back at your terminal where you can run kubectl commands on the kind cluster. Re-run kube-bench \u00b6 First delete the previous job: kubectl delete job kube-bench Run the kube-bench job, as before: kubectl apply -f https://raw.githubusercontent.com/aquasecurity/kube-bench/master/job.yaml Once the job has completed, get the test results from the logs kubectl logs $(kubectl get pods -l app=kube-bench -o name) This time you should see that test 4.2.6 passes. Congratulations, you have remediated a security setting on a Kubernetes node! Note This only remediates the running node, of course! If you are managing your own Kubernetes nodes, it would be better to update the configuration settings you use in deployment scripts, so that the nodes are configured to run from the outset with the settings you want. Running kube-bench through Starboard \u00b6 You can also use Starboard to run kube-bench and store the results in a Kubernetes CRD. $ kubectl starboard kube-bench $ kubectl get ciskubebenchreports -o yaml These results can be easily viewed using Octant and the Octant Starboard plugin. Using Starboard has the advantage that it will automatically run a kube-bench job on all the nodes in the cluster. Optional exercises \u00b6 If you download the job.yaml file used above, you can modify it to try some optional exercises. Run a specific test \u00b6 Sometimes you might want to run an individual test rather than the whole benchmark. For example, try to modify the command run in the job so that it only runs the test 4.2.6 that you remediated earlier. You can do this by specifying --check=4.2.6 as a parameter to the kube-bench command. Specify worker node tests only \u00b6 There are different CIS Kubernetes Benchmark tests for different node types in the cluster (control plane nodes, worker nodes, etcd nodes). On a managed Kubernetes system you might only have access to worker nodes, so you only need to run the tests that apply to those nodes. kube-bench tries to auto-detect which tests to run on any given node, but to keep things simple you may wish to specify worker node tests only. You might like to try out the job-node.yaml configuration which does just that.","title":"Secure settings"},{"location":"settings/#using-secure-kubernetes-settings","text":"When you install Kubernetes, there are numerous configuration settings that can affect security. In this section you'll learn about checking the settings in your cluster against the best practices advised by the Center for Internet Security.","title":"Using secure Kubernetes settings"},{"location":"settings/#the-cis-kubernetes-benchmark","text":"The CIS have many different benchmarks recommending how to configure software components with security best practices in mind. One such benchmark is for Kubernetes - in fact there are several editions for managed version of Kubernetes, like EKS or GKE, as well as for upstream Kubernetes installations. There are hundreds of recommendations in these benchmarks, so running them manually on every node would be a time-consuming process.","title":"The CIS Kubernetes Benchmark"},{"location":"settings/#running-benchmark-checks-with-kube-bench","text":"The open source tool kube-bench makes it easy to run the tests defined in the CIS Kubernetes benchmark. In this tutorial, you will use kube-bench to identify some insecure Kubernetes settings, and you'll remediate one of the settings to turn a failing test into a pass. You could run kube-bench in a cluster of your choice but for this tutorial we are showing it running in a kind (Kubernetes in Docker) single-node cluster that runs on your laptop as a Docker container.","title":"Running benchmark checks with kube-bench"},{"location":"settings/#run-kube-bench-on-the-kind-cluster","text":"Danger What we are about to do is TERRIBLE practice but it makes it easier to write a platform-independent set of instructions for this tutorial. Never run YAML directly from the internet like this in your production cluster - check what's in it first!","title":"Run kube-bench on the kind cluster"},{"location":"settings/#create-the-kube-bench-job","text":"kubectl apply -f https://raw.githubusercontent.com/aquasecurity/kube-bench/master/job.yaml You can watch the job until it has completed: kubectl get jobs --watch Hit Ctrl-C once the job has finished.","title":"Create the kube-bench job"},{"location":"settings/#get-the-job-output-from-the-logs","text":"The job applies the label app: kube-bench to the pod, so you can easily retrieve the logs like this: kubectl logs $(kubectl get pods -l app=kube-bench -o name) Scroll back through the logs to see how it is divided into sections, each with its own set of results, remediation recommendations, and a summary. Most of the tests pass but there are a few results marked with [WARN] or [FAIL] [FAIL] means that the test failed [WARN] indicates that you need to do something manually to verify whether the test should pass or not. For more detail on the output check the kube-bench documentation .","title":"Get the job output from the logs"},{"location":"settings/#remediate-a-test","text":"Note This tutorial was written using Kubernetes 1.18.2 and testing against the CIS Kubernetes Benchmark v1.5.1. If you are using a later version of Kubernetes, it's possible that the default configuration settings have changed and the results you get might not match what is described here. Scroll back through the results to find the result and (further down the results) the remediation information for the test 4.2.6. ... [FAIL] 4.2.6 Ensure that the --protect-kernel-defaults argument is set to true (Scored) ... 4.2.6 If using a Kubelet config file, edit the file to set protectKernelDefaults: true. If using command line arguments, edit the kubelet service file /etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and set the below parameter in KUBELET_SYSTEM_PODS_ARGS variable. --protect-kernel-defaults=true Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Kind uses a kubelet configuration file that lives at /var/lib/kubelet/config.yaml , so only the first line of the remediation text applies - you don't have to worry about editing the kubelet service file or restarting the service. Note When using kind, there is a Docker container running your control plane. This image for this container is based on Ubuntu so we can exec into the running container and then treat it much as if it were a virtual machine running a Kubernetes node.","title":"Remediate a test"},{"location":"settings/#edit-the-kubelet-configuration-file","text":"First, open a shell into the kind container. docker exec -it kind-control-plane bash Assuming that it doesn't already have an editor installed, you can add one. apt-get update apt-get install vim Edit the Kubelet config file vi /var/lib/kubelet/config.yaml Add the line protectKernelDefaults: true so that the file looks something like this: apiVersion: kubelet.config.k8s.io/v1beta1 authentication: anonymous: enabled: false ... nodeStatusUpdateFrequency: 0s protectKernelDefaults: true rotateCertificates: true ... Save the file. The kubelet will spot that the configuration has changed and update itself, but meanwhile you can exit to leave the container so that you are back at your terminal where you can run kubectl commands on the kind cluster.","title":"Edit the Kubelet configuration file"},{"location":"settings/#re-run-kube-bench","text":"First delete the previous job: kubectl delete job kube-bench Run the kube-bench job, as before: kubectl apply -f https://raw.githubusercontent.com/aquasecurity/kube-bench/master/job.yaml Once the job has completed, get the test results from the logs kubectl logs $(kubectl get pods -l app=kube-bench -o name) This time you should see that test 4.2.6 passes. Congratulations, you have remediated a security setting on a Kubernetes node! Note This only remediates the running node, of course! If you are managing your own Kubernetes nodes, it would be better to update the configuration settings you use in deployment scripts, so that the nodes are configured to run from the outset with the settings you want.","title":"Re-run kube-bench"},{"location":"settings/#running-kube-bench-through-starboard","text":"You can also use Starboard to run kube-bench and store the results in a Kubernetes CRD. $ kubectl starboard kube-bench $ kubectl get ciskubebenchreports -o yaml These results can be easily viewed using Octant and the Octant Starboard plugin. Using Starboard has the advantage that it will automatically run a kube-bench job on all the nodes in the cluster.","title":"Running kube-bench through Starboard"},{"location":"settings/#optional-exercises","text":"If you download the job.yaml file used above, you can modify it to try some optional exercises.","title":"Optional exercises"},{"location":"settings/#run-a-specific-test","text":"Sometimes you might want to run an individual test rather than the whole benchmark. For example, try to modify the command run in the job so that it only runs the test 4.2.6 that you remediated earlier. You can do this by specifying --check=4.2.6 as a parameter to the kube-bench command.","title":"Run a specific test"},{"location":"settings/#specify-worker-node-tests-only","text":"There are different CIS Kubernetes Benchmark tests for different node types in the cluster (control plane nodes, worker nodes, etcd nodes). On a managed Kubernetes system you might only have access to worker nodes, so you only need to run the tests that apply to those nodes. kube-bench tries to auto-detect which tests to run on any given node, but to keep things simple you may wish to specify worker node tests only. You might like to try out the job-node.yaml configuration which does just that.","title":"Specify worker node tests only"}]}