{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"This KubeCon EU 2020 tutorial will get you off the ground with Kubernetes security basics, using live demos and examples to work through yourself. We\u2019ll start with possible attack vectors, to help you map out the threat model that applies to your cluster, so you can figure out where you need to focus your efforts for security. We\u2019ll show you how to compromise a deployment with a pod running with a known vulnerability. Once you\u2019ve had the attacker\u2019s eye-view, we\u2019ll walk you through the most important techniques and open source tools to prevent compromise. Sign up now!","title":"Overview"},{"location":"compromise/","text":"A compromised pod \u00b6 In this section we deliberately introduce a container image with a known vulnerability so that you can enjoy the experience of exploiting it! Run a server vulnerable to Shellshock \u00b6 We're using a container with Shellshock , a vulnerability in bash that allows an attacker to remotely execute commands. To make it really easy, we're providing a Helm chart that installs the vulnerable deployment. DO NOT RUN THIS IN A REAL CLUSTER! helm install shellshockable https://lizrice.github.io/shellshockable/shellshockable-0.1.0.tgz This will run a deployment with a single pod. It might take a few seconds to pull the image so check that it's up and running (your pod name will be different): $ kubectl get pods NAME READY STATUS RESTARTS AGE shellshockable-d5b7d44b4-9rpzd 1/1 Running 0 70s In a separate terminal window, run port forwarding so we can make curl requests to this pod. This maps localhost port 8081 to the pod's port 80. kubectl port-forward $(kubectl get pods -l app.kubernetes.io/name=shellshockable -o name) 8081:80 You can now use curl (or a browser) to view the web service running on this pod. For example, if you open localhost:8081 in your web browser, you'll see the Apache2 default welcome page. There is also a script that will return the words \"Regular, expected output\" at the address localhost:8081/cgi-bin/shockme.cgi . If you look at the source for the script you'll see that it's executed using bash . This container is using a compromised version of bash with the Shellshock vulnerability, that an attacker can exploit to execute commands. First let's see the regular text is returned if we use curl to make the HTTPS request: $ curl localhost:8081/cgi-bin/shockme.cgi Regular, expected output You've seen the web server return content as expected. Now it's time to act like an attacker and exploit the Shellshock vulnerability. Exploit the vulnerability \u00b6 You can exploit Shellshock by passing a User-Agent header on the curl request with the -A parameter. This gets expanded as an environment variable by bash , and because of the Shellshock vulnerability, it can be used to execute arbitrary commands. For example curl -A \"() { :; }; echo \\\"Content-type: text/plain\\\"; echo; /bin/cat /etc/passwd\" localhost:8081/cgi-bin/shockme.cgi Instead of running the CGI script as normal, this reponds to the HTTP request with the contents of /etc/passwd ! Preventing this attack \u00b6 The safest way to prevent this attack is to ensure the vulnerable version of bash isn't included in the container image before it's deployed. This is done with container image scanning . Image scanning can only detect known, published vulnerabilities. You can also limit the likely damage of as-yet-unknown compromises by configuring containers to run more securely and using policies to enforce safer configuration. Optional questions and exercises \u00b6 If you have the time and interest, here are some additional things you might like to think about. Try running some other commands as if you were an attacker! For example, you could find out what environment variables are set see what user ID you're running as explore the contents of the filesystem to see what is available Where does this /etc/passwd file come from - the host or the container? What would happen if you mounted files from the host into this container? Take a look at the Dockerfile that builds the container image used in this example. It deliberately installed an old, vulnerable version of bash . Try running a deployment with an up-to-date version of apache2 without the vulnerability, and check that you can't exploit it in the same way.","title":"Compromise a pod!"},{"location":"compromise/#a-compromised-pod","text":"In this section we deliberately introduce a container image with a known vulnerability so that you can enjoy the experience of exploiting it!","title":"A compromised pod"},{"location":"compromise/#run-a-server-vulnerable-to-shellshock","text":"We're using a container with Shellshock , a vulnerability in bash that allows an attacker to remotely execute commands. To make it really easy, we're providing a Helm chart that installs the vulnerable deployment. DO NOT RUN THIS IN A REAL CLUSTER! helm install shellshockable https://lizrice.github.io/shellshockable/shellshockable-0.1.0.tgz This will run a deployment with a single pod. It might take a few seconds to pull the image so check that it's up and running (your pod name will be different): $ kubectl get pods NAME READY STATUS RESTARTS AGE shellshockable-d5b7d44b4-9rpzd 1/1 Running 0 70s In a separate terminal window, run port forwarding so we can make curl requests to this pod. This maps localhost port 8081 to the pod's port 80. kubectl port-forward $(kubectl get pods -l app.kubernetes.io/name=shellshockable -o name) 8081:80 You can now use curl (or a browser) to view the web service running on this pod. For example, if you open localhost:8081 in your web browser, you'll see the Apache2 default welcome page. There is also a script that will return the words \"Regular, expected output\" at the address localhost:8081/cgi-bin/shockme.cgi . If you look at the source for the script you'll see that it's executed using bash . This container is using a compromised version of bash with the Shellshock vulnerability, that an attacker can exploit to execute commands. First let's see the regular text is returned if we use curl to make the HTTPS request: $ curl localhost:8081/cgi-bin/shockme.cgi Regular, expected output You've seen the web server return content as expected. Now it's time to act like an attacker and exploit the Shellshock vulnerability.","title":"Run a server vulnerable to Shellshock"},{"location":"compromise/#exploit-the-vulnerability","text":"You can exploit Shellshock by passing a User-Agent header on the curl request with the -A parameter. This gets expanded as an environment variable by bash , and because of the Shellshock vulnerability, it can be used to execute arbitrary commands. For example curl -A \"() { :; }; echo \\\"Content-type: text/plain\\\"; echo; /bin/cat /etc/passwd\" localhost:8081/cgi-bin/shockme.cgi Instead of running the CGI script as normal, this reponds to the HTTP request with the contents of /etc/passwd !","title":"Exploit the vulnerability"},{"location":"compromise/#preventing-this-attack","text":"The safest way to prevent this attack is to ensure the vulnerable version of bash isn't included in the container image before it's deployed. This is done with container image scanning . Image scanning can only detect known, published vulnerabilities. You can also limit the likely damage of as-yet-unknown compromises by configuring containers to run more securely and using policies to enforce safer configuration.","title":"Preventing this attack"},{"location":"compromise/#optional-questions-and-exercises","text":"If you have the time and interest, here are some additional things you might like to think about. Try running some other commands as if you were an attacker! For example, you could find out what environment variables are set see what user ID you're running as explore the contents of the filesystem to see what is available Where does this /etc/passwd file come from - the host or the container? What would happen if you mounted files from the host into this container? Take a look at the Dockerfile that builds the container image used in this example. It deliberately installed an old, vulnerable version of bash . Try running a deployment with an up-to-date version of apache2 without the vulnerability, and check that you can't exploit it in the same way.","title":"Optional questions and exercises"},{"location":"conclusions/","text":"Conclusions \u00b6 We hope this tutorial has introduced you to some practical steps you can take to make your Kubernetes deployments more secure. Thank you for your time! Further reading \u00b6 You'll find more details and further resources about Kubernetes Security here including a link to download an electronic copy of our book here . If you'd like to dive into more technical details, you might also like to check out Liz's book on Container Security .","title":"Conclusion"},{"location":"conclusions/#conclusions","text":"We hope this tutorial has introduced you to some practical steps you can take to make your Kubernetes deployments more secure. Thank you for your time!","title":"Conclusions"},{"location":"conclusions/#further-reading","text":"You'll find more details and further resources about Kubernetes Security here including a link to download an electronic copy of our book here . If you'd like to dive into more technical details, you might also like to check out Liz's book on Container Security .","title":"Further reading"},{"location":"gitops/","text":"GitOps \u00b6 Concept \u00b6 Using Flux \u00b6 Using ArgoCD \u00b6","title":"GitOps"},{"location":"gitops/#gitops","text":"","title":"GitOps"},{"location":"gitops/#concept","text":"","title":"Concept"},{"location":"gitops/#using-flux","text":"","title":"Using Flux"},{"location":"gitops/#using-argocd","text":"","title":"Using ArgoCD"},{"location":"introduction/","text":"Introduction \u00b6 If you want to follow along with the hands-on examples in this tutorial, you'll need a Kubernetes cluster and Helm to get started. You'll find instructions in the Preparation page. Before we get to the examples, let's review some of the different ways that an attacker might compromise a Kubernetes cluster. Kubernetes attack vectors \u00b6 We won't have time to explore all of these in detail but we will be covering some of the most important topics. An attacker might take advantage of vulnerabilities in your application code. You can go a long way to address this by scanning container images for vulnerabilities If an attacker gets a foothold within a container, you want to prevent them from escaping the container to access the host or other components, by configuring container images with security in mind, and checking them with policies The Kubernetes components and APIs offer more potential surfaces for attack, so you want to checking your Kubernetes configuration for settings that might leave your deployment vulnerable. At the root of many security issues lies human error, as well as bad actors. You can limit human access to the cluster, and thus enhancing security using GitOps","title":"Introduction"},{"location":"introduction/#introduction","text":"If you want to follow along with the hands-on examples in this tutorial, you'll need a Kubernetes cluster and Helm to get started. You'll find instructions in the Preparation page. Before we get to the examples, let's review some of the different ways that an attacker might compromise a Kubernetes cluster.","title":"Introduction"},{"location":"introduction/#kubernetes-attack-vectors","text":"We won't have time to explore all of these in detail but we will be covering some of the most important topics. An attacker might take advantage of vulnerabilities in your application code. You can go a long way to address this by scanning container images for vulnerabilities If an attacker gets a foothold within a container, you want to prevent them from escaping the container to access the host or other components, by configuring container images with security in mind, and checking them with policies The Kubernetes components and APIs offer more potential surfaces for attack, so you want to checking your Kubernetes configuration for settings that might leave your deployment vulnerable. At the root of many security issues lies human error, as well as bad actors. You can limit human access to the cluster, and thus enhancing security using GitOps","title":"Kubernetes attack vectors"},{"location":"policies/","text":"Configuring pods to run securely \u00b6 Least privileges \u00b6 http://canihaznonprivilegedcontainers.info/ Network policies \u00b6 https://banzaicloud.com/blog/network-policy/ https://medium.com/@tufin/best-practices-for-kubernetes-network-policies-2b643c4b1aa OPA in action \u00b6 what is it mini example via https://play.openpolicyagent.org/ OPA Gatekeeper","title":"Policies"},{"location":"policies/#configuring-pods-to-run-securely","text":"","title":"Configuring pods to run securely"},{"location":"policies/#least-privileges","text":"http://canihaznonprivilegedcontainers.info/","title":"Least privileges"},{"location":"policies/#network-policies","text":"https://banzaicloud.com/blog/network-policy/ https://medium.com/@tufin/best-practices-for-kubernetes-network-policies-2b643c4b1aa","title":"Network policies"},{"location":"policies/#opa-in-action","text":"what is it mini example via https://play.openpolicyagent.org/ OPA Gatekeeper","title":"OPA in action"},{"location":"preparation/","text":"Preparation \u00b6 Create a Kubernetes cluster \u00b6 To follow along with the practical examples in this tutorial you'll need a Kubernetes cluster that you can experiment with. Since at times you will be deploying insecure code, please don't use your production cluster! You can run a cluster locally on your laptop, for example using Kind - Kubernetes IN Docker . We'll also be using Helm to run software on the Kind cluster. Install kind \u00b6 You can skip this step if you already have an up-to-date installation of kind . On MacOS using Homebrew: brew install kind On MacOS / Linux: curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.8.1/kind-$(uname)-amd64 chmod +x ./kind mv ./kind /some-dir-in-your-PATH/kind On Windows using Chocolatey: choco install kind For more details see the kind quickstart guide . Create the kind cluster \u00b6 kind create cluster Once it's up and running, check that you can see the node is up and running: kubectl get nodes This should show something like this: NAME STATUS ROLES AGE VERSION kind-control-plane Ready master 78m v1.18.2 Great! You have a Kubernetes cluster running locally that you can experiment with. Install Helm \u00b6 If you don't already have Helm on your laptop, you'll want to install that too. Find full instructions in the Helm documentation or here is a quick guide: On MacOS using Homebrew: brew install helm On MacOS / Linux: curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 chmod +x get_helm.sh ./get_helm.sh On Windows using Chocolatey: choco install kubernetes-helm If you have a fresh Kind installation there won't be any Helm charts installed yet, so a helm ls will return an empty list: $ helm ls NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION","title":"Preparation"},{"location":"preparation/#preparation","text":"","title":"Preparation"},{"location":"preparation/#create-a-kubernetes-cluster","text":"To follow along with the practical examples in this tutorial you'll need a Kubernetes cluster that you can experiment with. Since at times you will be deploying insecure code, please don't use your production cluster! You can run a cluster locally on your laptop, for example using Kind - Kubernetes IN Docker . We'll also be using Helm to run software on the Kind cluster.","title":"Create a Kubernetes cluster"},{"location":"preparation/#install-kind","text":"You can skip this step if you already have an up-to-date installation of kind . On MacOS using Homebrew: brew install kind On MacOS / Linux: curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.8.1/kind-$(uname)-amd64 chmod +x ./kind mv ./kind /some-dir-in-your-PATH/kind On Windows using Chocolatey: choco install kind For more details see the kind quickstart guide .","title":"Install kind"},{"location":"preparation/#create-the-kind-cluster","text":"kind create cluster Once it's up and running, check that you can see the node is up and running: kubectl get nodes This should show something like this: NAME STATUS ROLES AGE VERSION kind-control-plane Ready master 78m v1.18.2 Great! You have a Kubernetes cluster running locally that you can experiment with.","title":"Create the kind cluster"},{"location":"preparation/#install-helm","text":"If you don't already have Helm on your laptop, you'll want to install that too. Find full instructions in the Helm documentation or here is a quick guide: On MacOS using Homebrew: brew install helm On MacOS / Linux: curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 chmod +x get_helm.sh ./get_helm.sh On Windows using Chocolatey: choco install kubernetes-helm If you have a fresh Kind installation there won't be any Helm charts installed yet, so a helm ls will return an empty list: $ helm ls NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION","title":"Install Helm"},{"location":"scanning/","text":"Vulnerability scanning \u00b6 The Shellshock vulnerability demonstrated is a serious vulnerability, but it's just one of thousands of known, exploitable vulnerabilities that range from negligible right up to critical in severity. Most organizations have a policy of not deploying software with known vulnerabilities over a certain severity threshold. A vulnerability scanner can show you which vulnerabilities are present in your container images. This data can inform policies to prevent deploying vulnerable containers that an attacker could exploit. There are several solutions for vulnerability scanning available, and in this workshop we will use the open source scanner Trivy . Running Trivy on the desktop \u00b6 Let's start by installing Trivy so we can run it locally. Install Trivy \u00b6 On MacOS using Homebrew: brew install aquasecurity/trivy/trivy On MacOS / Linux: curl -sfL https://raw.githubusercontent.com/aquasecurity/trivy/master/contrib/install.sh | sh -s -- -b /usr/local/bin Trivy does not have a native binary for Windows, but you can run Trivy as a container . For more details see the installation instructions . Run Trivy to scan images \u00b6 To scan an image simply run trivy image <image> . For example, to find the vulnerabilities in the image used in the Shellshock-compromised pod, run this: trivy image lizrice/shellshockable:0.1.0 The Shellshock vulnerability is classed as HIGH severity, and it has the identifier CVE-2014-6271 . If you are seeing a lot of results you might want to filter out the lower severity results. For example: trivy image --severity=CRITICAL,HIGH,MEDIUM lizrice/shellshockable:0.1.0 For comparison, try this version of the image that does not have the Shellshock vulnerability: trivy image --severity=CRITICAL,HIGH,MEDIUM lizrice/shellshockable:0.3.0 Now try Trivy on your favourite images! Including vulnerability scanning in CI/CD \u00b6 One way to \"shift left\" security is to include vulnerability scanning as an automatic step in CI/CD. Your build steps might be something like this: Compile code Run tests Build Dockerfile into container image Scan image for vulnerabilities Push image to container registry If any step fails, the pipeline is stopped - so if a vulnerability is found, the scanning stage fails, and the image doesn't get pushed to the registry. Trivy has some settings that make it a great fit in CI/CD systems: --light downloads a smaller vulnerability database that omits some details such as the text description of each vulnerability --severity=CRITICAL,HIGH,MEDIUM only checks for vulnerabilities of MEDIUM or higher severity --exit-code=1 returns 1 if any vulnerabilties are found (above the threshold defined by --severity ). This non-zero exit code is interpreted as a failure by the CI/CD system. Take a look at these examples for integrating vulnerability scanning into various CI/CD systems . TODO! In the GitOps section, if attendees will set up a Git repo, maybe we could get them to try the Trivy action there? Scanning as part of admission control \u00b6 To prevent deploying a container image with known vulnerabilities, you can use an admission controller that either scans the image, or retrieves the result from a previous scan and denies admission if the image is vulnerable. Teppei Fukuda has a session at KubeCon this week showing the use of OPA to make policy checks at the admission control stage. Scanning running Kubernetes workloads \u00b6 Scanning in the CI/CD pipeline can prevent you from storing a vulnerable image in the registry, and admission control can prevent you from deploying an image with known vulnerabilities. But you may also want the ability to check the container images used by your live workloads. New vulnerabilities are found all the time. An image that you scan today might not have any vulnerablities, but a researcher may find a new issue that means the same image will fail the scan tomorrow. Starboard is a tool for running security tools, including Trivy, within your Kubernetes cluster. This is an easy way to create and view scans of the container images used by your running workloads. Want to give it a try? Install Starboard \u00b6 If you don't already have the krew plugin manager installed, follow the instructions here Install Starboard as a kubectl plugin: kubectl krew install starboard Check that it's working kubectl starboard help On Mac you may get warnings about not being able to verify the developer - see this guide to resolve them . Initialize Starboard kubectl starboard init Check that this has created some CRDs kubectl get crds Find vulnerabilities \u00b6 Starboard uses Trivy to populate some vulnerabilities CRs that relate to running workloads. Let's find the vulnerabilities in the deployment we created for the Shellshock demo. kubectl starboard find vulns --delete-scan-job = false deployment/shellshockable This will take a few seconds. The --delete-scan-job=false parameter allows us to inspect the job that Starboard creates to run Trivy. But first let's look at the results: kubectl starboard get vulns deployment/shellshockable This outputs all the vulnerability information (including the details about Shellshock, with ID CVE-2014-6271). These details have been stored in a vulns resource that you can view with a regular kubectl get command: $ kubectl get vulns --show-labels NAME AGE LABELS c9b156db-ab92-4b32-a006-efc54312a8c1 4m14s starboard.container.name = shellshockable,starboard.resource.kind = Deployment,starboard.resource.name = shellshockable,starboard.resource.namespace = default The labels indicate which resource this vulnerability report applies to. TODO! On master, but not yet released as a binary, we have additional info with -o wide As mentioned above, Starboard creates a Kubernetes job that runs Trivy over the container images defined for the workload. In this case, it establishes that the shellshockable deployment uses the container image lizrice/shellshockable:0.1.0 and runs Trivy over that image. If you specified --delete-scan-job=false you can inspect the completed job to see the Trivy command that it ran: $ kubectl get jobs -n starboard -o wide NAME COMPLETIONS DURATION AGE CONTAINERS IMAGES SELECTOR 637021cc-05d3-497a-a399-7b2ef0b1ebfd 1 /1 38s 4m46s shellshockable docker.io/aquasec/trivy:0.9.1 controller-uid = 0f5e6fac-13a9-438e-9fb4-65ac26fda82f $ kubectl describe job -n starboard 637021cc-05d3-497a-a399-7b2ef0b1ebfd ... Containers: shellshockable: Image: docker.io/aquasec/trivy:0.9.1 Port: <none> Host Port: <none> Command: trivy Args: --skip-update --cache-dir /var/lib/trivy --no-progress --format json lizrice/shellshockable:0.1.0 ... Starboard makes it easy to run Trivy over your running workloads. There is also an Octant plugin so you can view the vulnerability results through the Octant UI. You can check out the instructions for this here if you want to try it out.","title":"Scanning"},{"location":"scanning/#vulnerability-scanning","text":"The Shellshock vulnerability demonstrated is a serious vulnerability, but it's just one of thousands of known, exploitable vulnerabilities that range from negligible right up to critical in severity. Most organizations have a policy of not deploying software with known vulnerabilities over a certain severity threshold. A vulnerability scanner can show you which vulnerabilities are present in your container images. This data can inform policies to prevent deploying vulnerable containers that an attacker could exploit. There are several solutions for vulnerability scanning available, and in this workshop we will use the open source scanner Trivy .","title":"Vulnerability scanning"},{"location":"scanning/#running-trivy-on-the-desktop","text":"Let's start by installing Trivy so we can run it locally.","title":"Running Trivy on the desktop"},{"location":"scanning/#install-trivy","text":"On MacOS using Homebrew: brew install aquasecurity/trivy/trivy On MacOS / Linux: curl -sfL https://raw.githubusercontent.com/aquasecurity/trivy/master/contrib/install.sh | sh -s -- -b /usr/local/bin Trivy does not have a native binary for Windows, but you can run Trivy as a container . For more details see the installation instructions .","title":"Install Trivy"},{"location":"scanning/#run-trivy-to-scan-images","text":"To scan an image simply run trivy image <image> . For example, to find the vulnerabilities in the image used in the Shellshock-compromised pod, run this: trivy image lizrice/shellshockable:0.1.0 The Shellshock vulnerability is classed as HIGH severity, and it has the identifier CVE-2014-6271 . If you are seeing a lot of results you might want to filter out the lower severity results. For example: trivy image --severity=CRITICAL,HIGH,MEDIUM lizrice/shellshockable:0.1.0 For comparison, try this version of the image that does not have the Shellshock vulnerability: trivy image --severity=CRITICAL,HIGH,MEDIUM lizrice/shellshockable:0.3.0 Now try Trivy on your favourite images!","title":"Run Trivy to scan images"},{"location":"scanning/#including-vulnerability-scanning-in-cicd","text":"One way to \"shift left\" security is to include vulnerability scanning as an automatic step in CI/CD. Your build steps might be something like this: Compile code Run tests Build Dockerfile into container image Scan image for vulnerabilities Push image to container registry If any step fails, the pipeline is stopped - so if a vulnerability is found, the scanning stage fails, and the image doesn't get pushed to the registry. Trivy has some settings that make it a great fit in CI/CD systems: --light downloads a smaller vulnerability database that omits some details such as the text description of each vulnerability --severity=CRITICAL,HIGH,MEDIUM only checks for vulnerabilities of MEDIUM or higher severity --exit-code=1 returns 1 if any vulnerabilties are found (above the threshold defined by --severity ). This non-zero exit code is interpreted as a failure by the CI/CD system. Take a look at these examples for integrating vulnerability scanning into various CI/CD systems . TODO! In the GitOps section, if attendees will set up a Git repo, maybe we could get them to try the Trivy action there?","title":"Including vulnerability scanning in CI/CD"},{"location":"scanning/#scanning-as-part-of-admission-control","text":"To prevent deploying a container image with known vulnerabilities, you can use an admission controller that either scans the image, or retrieves the result from a previous scan and denies admission if the image is vulnerable. Teppei Fukuda has a session at KubeCon this week showing the use of OPA to make policy checks at the admission control stage.","title":"Scanning as part of admission control"},{"location":"scanning/#scanning-running-kubernetes-workloads","text":"Scanning in the CI/CD pipeline can prevent you from storing a vulnerable image in the registry, and admission control can prevent you from deploying an image with known vulnerabilities. But you may also want the ability to check the container images used by your live workloads. New vulnerabilities are found all the time. An image that you scan today might not have any vulnerablities, but a researcher may find a new issue that means the same image will fail the scan tomorrow. Starboard is a tool for running security tools, including Trivy, within your Kubernetes cluster. This is an easy way to create and view scans of the container images used by your running workloads. Want to give it a try?","title":"Scanning running Kubernetes workloads"},{"location":"scanning/#install-starboard","text":"If you don't already have the krew plugin manager installed, follow the instructions here Install Starboard as a kubectl plugin: kubectl krew install starboard Check that it's working kubectl starboard help On Mac you may get warnings about not being able to verify the developer - see this guide to resolve them . Initialize Starboard kubectl starboard init Check that this has created some CRDs kubectl get crds","title":"Install Starboard"},{"location":"scanning/#find-vulnerabilities","text":"Starboard uses Trivy to populate some vulnerabilities CRs that relate to running workloads. Let's find the vulnerabilities in the deployment we created for the Shellshock demo. kubectl starboard find vulns --delete-scan-job = false deployment/shellshockable This will take a few seconds. The --delete-scan-job=false parameter allows us to inspect the job that Starboard creates to run Trivy. But first let's look at the results: kubectl starboard get vulns deployment/shellshockable This outputs all the vulnerability information (including the details about Shellshock, with ID CVE-2014-6271). These details have been stored in a vulns resource that you can view with a regular kubectl get command: $ kubectl get vulns --show-labels NAME AGE LABELS c9b156db-ab92-4b32-a006-efc54312a8c1 4m14s starboard.container.name = shellshockable,starboard.resource.kind = Deployment,starboard.resource.name = shellshockable,starboard.resource.namespace = default The labels indicate which resource this vulnerability report applies to. TODO! On master, but not yet released as a binary, we have additional info with -o wide As mentioned above, Starboard creates a Kubernetes job that runs Trivy over the container images defined for the workload. In this case, it establishes that the shellshockable deployment uses the container image lizrice/shellshockable:0.1.0 and runs Trivy over that image. If you specified --delete-scan-job=false you can inspect the completed job to see the Trivy command that it ran: $ kubectl get jobs -n starboard -o wide NAME COMPLETIONS DURATION AGE CONTAINERS IMAGES SELECTOR 637021cc-05d3-497a-a399-7b2ef0b1ebfd 1 /1 38s 4m46s shellshockable docker.io/aquasec/trivy:0.9.1 controller-uid = 0f5e6fac-13a9-438e-9fb4-65ac26fda82f $ kubectl describe job -n starboard 637021cc-05d3-497a-a399-7b2ef0b1ebfd ... Containers: shellshockable: Image: docker.io/aquasec/trivy:0.9.1 Port: <none> Host Port: <none> Command: trivy Args: --skip-update --cache-dir /var/lib/trivy --no-progress --format json lizrice/shellshockable:0.1.0 ... Starboard makes it easy to run Trivy over your running workloads. There is also an Octant plugin so you can view the vulnerability results through the Octant UI. You can check out the instructions for this here if you want to try it out.","title":"Find vulnerabilities"},{"location":"settings/","text":"Using secure Kubernetes settings \u00b6 When you install Kubernetes, there are numerous configuration settings that can affect security. In this section you'll learn about checking the settings in your cluster against the best practices advised by the Center for Internet Security. The CIS Kubernetes Benchmark \u00b6 The CIS have many different benchmarks recommending how to configure software components with security best practices in mind. One such benchmark is for Kubernetes - in fact there are several editions for managed version of Kubernetes, like EKS or GKE, as well as for upstream Kubernetes installations. There are hundreds of recommendations in these benchmarks, so running them manually on every node would be a time-consuming process. Running benchmark checks with kube-bench \u00b6 The open source tool kube-bench makes it easy to run the tests defined in the CIS Kubernetes benchmark. In this tutorial, you will use kube-bench to identify some insecure Kubernetes settings, and you'll remediate one of the settings to turn a failing test into a pass. You could run kube-bench in a cluster of your choice but for this tutorial we are showing it running in a kind (Kubernetes in Docker) single-node cluster that runs on your laptop as a Docker container. Run kube-bench on the kind cluster \u00b6 !!! What we are about to do is TERRIBLE practice but it makes it easier for us to write a platform-independent set of instructions for this tutorial. Never run YAML directly from the internet like this in your production cluster - check what's in it first! Create the kube-bench job \u00b6 kubectl apply -f https://raw.githubusercontent.com/aquasecurity/kube-bench/master/job.yaml You can watch the job until it has completed: kubectl get jobs --watch Hit Ctrl-C once the job has finished. Get the job output from the logs \u00b6 The job applies the label app: kube-bench to the pod, so you can easily retrieve the logs like this: kubectl logs $(kubectl get pods -l app=kube-bench -o name) Scroll back through the logs to see how it is divided into sections, each with its own set of results, remediation recommendations, and a summary. Most of the tests pass but there are a few results marked with [WARN] or [FAIL] [FAIL] means that the test failed [WARN] indicates that you need to do something manually to verify whether the test should pass or not. For more detail on the output check the kube-bench documentation . Remediate a test \u00b6 !!! This tutorial was written using Kubernetes 1.18.2 and testing against the CIS Kubernetes Benchmark v1.5.1. If you are using a later version of Kubernetes, it's possible that the default configuration settings have changed and the results you get might not match what is described here. Scroll back through the results to find the result and (further down the results) the remediation information for the test 4.2.6. ... [FAIL] 4.2.6 Ensure that the --protect-kernel-defaults argument is set to true (Scored) ... 4.2.6 If using a Kubelet config file, edit the file to set protectKernelDefaults: true. If using command line arguments, edit the kubelet service file /etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and set the below parameter in KUBELET_SYSTEM_PODS_ARGS variable. --protect-kernel-defaults=true Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Kind uses a kubelet configuration file that lives at /var/lib/kubelet/config.yaml , so only the first line of the remediation text applies - you don't have to worry about editing the kubelet service file or restarting the service. !!! When using kind, there is a Docker container running your control plane. This image for this container is based on Ubuntu so we can exec into the running container and then treat it much as if it were a virtual machine running a Kubernetes node. Edit the Kubelet configuration file \u00b6 First, open a shell into the kind container. docker exec -it kind-control-plane bash Assuming that it doesn't already have an editor installed, you can add one. apt-get update apt-get install vim Edit the Kubelet config file vi /var/lib/kubelet/config.yaml Add the line protectKernelDefaults: true so that the file looks something like this: apiVersion: kubelet.config.k8s.io/v1beta1 authentication: anonymous: enabled: false ... nodeStatusUpdateFrequency: 0s protectKernelDefaults: true rotateCertificates: true ... Save the file. The kubelet will spot that the configuration has changed and update itself, but meanwhile you can exit to leave the container so that you are back at your terminal where you can run kubectl commands on the kind cluster. Re-run kube-bench \u00b6 First delete the previous job: kubectl delete job kube-bench Run the kube-bench job, as before: kubectl apply -f https://raw.githubusercontent.com/aquasecurity/kube-bench/master/job.yaml Once the job has completed, get the test results from the logs kubectl logs $(kubectl get pods -l app=kube-bench -o name) This time you should see that test 4.2.6 passes. Congratulations, you have remediated a security setting on a Kubernetes node! !!! This only remediates the running node, of course! If you are managing your own Kubernetes nodes, it would be better to update the configuration settings you use in deployment scripts, so that the nodes are configured to run from the outset with the settings you want. Running kube-bench through Starboard \u00b6 You can also use Starboard to run kube-bench and store the results in a Kubernetes CRD. $ kubectl starboard kube-bench $ kubectl get ciskubebenchreports -o yaml These results can be easily viewed using Octant and the Octant Starboard plugin. Using Starboard has the advantage that it will automatically run a kube-bench job on all the nodes in the cluster. Optional exercises \u00b6 If you download the job.yaml file used above, you can modify it to try some optional exercises. Run a specific test \u00b6 Sometimes you might want to run an individual test rather than the whole benchmark. For example, try to modify the command run in the job so that it only runs the test 4.2.6 that you remediated earlier. You can do this by specifying --check=4.2.6 as a parameter to the kube-bench command. Specify worker node tests only \u00b6 There are different CIS Kubernetes Benchmark tests for different node types in the cluster (master nodes, worker nodes, etcd nodes). On a managed Kubernetes system you might only have access to worker nodes, so you only need to run the tests that apply to those nodes. kube-bench tries to auto-detect which tests to run on any given node, but to keep things simple you may wish to specify worker node tests only. You might like to try out the job-node.yaml configuration which does just that.","title":"Secure settings"},{"location":"settings/#using-secure-kubernetes-settings","text":"When you install Kubernetes, there are numerous configuration settings that can affect security. In this section you'll learn about checking the settings in your cluster against the best practices advised by the Center for Internet Security.","title":"Using secure Kubernetes settings"},{"location":"settings/#the-cis-kubernetes-benchmark","text":"The CIS have many different benchmarks recommending how to configure software components with security best practices in mind. One such benchmark is for Kubernetes - in fact there are several editions for managed version of Kubernetes, like EKS or GKE, as well as for upstream Kubernetes installations. There are hundreds of recommendations in these benchmarks, so running them manually on every node would be a time-consuming process.","title":"The CIS Kubernetes Benchmark"},{"location":"settings/#running-benchmark-checks-with-kube-bench","text":"The open source tool kube-bench makes it easy to run the tests defined in the CIS Kubernetes benchmark. In this tutorial, you will use kube-bench to identify some insecure Kubernetes settings, and you'll remediate one of the settings to turn a failing test into a pass. You could run kube-bench in a cluster of your choice but for this tutorial we are showing it running in a kind (Kubernetes in Docker) single-node cluster that runs on your laptop as a Docker container.","title":"Running benchmark checks with kube-bench"},{"location":"settings/#run-kube-bench-on-the-kind-cluster","text":"!!! What we are about to do is TERRIBLE practice but it makes it easier for us to write a platform-independent set of instructions for this tutorial. Never run YAML directly from the internet like this in your production cluster - check what's in it first!","title":"Run kube-bench on the kind cluster"},{"location":"settings/#create-the-kube-bench-job","text":"kubectl apply -f https://raw.githubusercontent.com/aquasecurity/kube-bench/master/job.yaml You can watch the job until it has completed: kubectl get jobs --watch Hit Ctrl-C once the job has finished.","title":"Create the kube-bench job"},{"location":"settings/#get-the-job-output-from-the-logs","text":"The job applies the label app: kube-bench to the pod, so you can easily retrieve the logs like this: kubectl logs $(kubectl get pods -l app=kube-bench -o name) Scroll back through the logs to see how it is divided into sections, each with its own set of results, remediation recommendations, and a summary. Most of the tests pass but there are a few results marked with [WARN] or [FAIL] [FAIL] means that the test failed [WARN] indicates that you need to do something manually to verify whether the test should pass or not. For more detail on the output check the kube-bench documentation .","title":"Get the job output from the logs"},{"location":"settings/#remediate-a-test","text":"!!! This tutorial was written using Kubernetes 1.18.2 and testing against the CIS Kubernetes Benchmark v1.5.1. If you are using a later version of Kubernetes, it's possible that the default configuration settings have changed and the results you get might not match what is described here. Scroll back through the results to find the result and (further down the results) the remediation information for the test 4.2.6. ... [FAIL] 4.2.6 Ensure that the --protect-kernel-defaults argument is set to true (Scored) ... 4.2.6 If using a Kubelet config file, edit the file to set protectKernelDefaults: true. If using command line arguments, edit the kubelet service file /etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and set the below parameter in KUBELET_SYSTEM_PODS_ARGS variable. --protect-kernel-defaults=true Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Kind uses a kubelet configuration file that lives at /var/lib/kubelet/config.yaml , so only the first line of the remediation text applies - you don't have to worry about editing the kubelet service file or restarting the service. !!! When using kind, there is a Docker container running your control plane. This image for this container is based on Ubuntu so we can exec into the running container and then treat it much as if it were a virtual machine running a Kubernetes node.","title":"Remediate a test"},{"location":"settings/#edit-the-kubelet-configuration-file","text":"First, open a shell into the kind container. docker exec -it kind-control-plane bash Assuming that it doesn't already have an editor installed, you can add one. apt-get update apt-get install vim Edit the Kubelet config file vi /var/lib/kubelet/config.yaml Add the line protectKernelDefaults: true so that the file looks something like this: apiVersion: kubelet.config.k8s.io/v1beta1 authentication: anonymous: enabled: false ... nodeStatusUpdateFrequency: 0s protectKernelDefaults: true rotateCertificates: true ... Save the file. The kubelet will spot that the configuration has changed and update itself, but meanwhile you can exit to leave the container so that you are back at your terminal where you can run kubectl commands on the kind cluster.","title":"Edit the Kubelet configuration file"},{"location":"settings/#re-run-kube-bench","text":"First delete the previous job: kubectl delete job kube-bench Run the kube-bench job, as before: kubectl apply -f https://raw.githubusercontent.com/aquasecurity/kube-bench/master/job.yaml Once the job has completed, get the test results from the logs kubectl logs $(kubectl get pods -l app=kube-bench -o name) This time you should see that test 4.2.6 passes. Congratulations, you have remediated a security setting on a Kubernetes node! !!! This only remediates the running node, of course! If you are managing your own Kubernetes nodes, it would be better to update the configuration settings you use in deployment scripts, so that the nodes are configured to run from the outset with the settings you want.","title":"Re-run kube-bench"},{"location":"settings/#running-kube-bench-through-starboard","text":"You can also use Starboard to run kube-bench and store the results in a Kubernetes CRD. $ kubectl starboard kube-bench $ kubectl get ciskubebenchreports -o yaml These results can be easily viewed using Octant and the Octant Starboard plugin. Using Starboard has the advantage that it will automatically run a kube-bench job on all the nodes in the cluster.","title":"Running kube-bench through Starboard"},{"location":"settings/#optional-exercises","text":"If you download the job.yaml file used above, you can modify it to try some optional exercises.","title":"Optional exercises"},{"location":"settings/#run-a-specific-test","text":"Sometimes you might want to run an individual test rather than the whole benchmark. For example, try to modify the command run in the job so that it only runs the test 4.2.6 that you remediated earlier. You can do this by specifying --check=4.2.6 as a parameter to the kube-bench command.","title":"Run a specific test"},{"location":"settings/#specify-worker-node-tests-only","text":"There are different CIS Kubernetes Benchmark tests for different node types in the cluster (master nodes, worker nodes, etcd nodes). On a managed Kubernetes system you might only have access to worker nodes, so you only need to run the tests that apply to those nodes. kube-bench tries to auto-detect which tests to run on any given node, but to keep things simple you may wish to specify worker node tests only. You might like to try out the job-node.yaml configuration which does just that.","title":"Specify worker node tests only"}]}